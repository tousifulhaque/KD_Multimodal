{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9dde82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dad3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f344b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  wandb_config import sweep_config\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "from model import Transformer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow_addons.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, Callback\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Add, Dense, LayerNormalization, Normalization , Masking, GlobalAveragePooling1D, Conv1D, Dropout, MultiHeadAttention, Layer\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, Callback, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.metrics import Recall, Precision , AUC\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83288736",
   "metadata": {},
   "source": [
    "## Lr Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9b8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "# from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# class PrintLR(Callback):\n",
    "#     def on_epoch_end(self, epoch, logs = None):\n",
    "\n",
    "\n",
    "def cosine_schedule(base_lr, total_steps, warmup_steps ):\n",
    "    def step_fn(epoch):\n",
    "        lr = base_lr\n",
    "        epoch = 1\n",
    "\n",
    "        progress = (epoch - warmup_steps) / float(total_steps -  warmup_steps)\n",
    "\n",
    "        progress = tf.clip_by_value(progress, 0.0, 1.0)\n",
    "\n",
    "        lr = lr * 0.5 * (1.0 + tf.cos(math.pi * progress))\n",
    "        \n",
    "        if warmup_steps:\n",
    "            lr = lr * tf.minimum(1.0 , epoch/warmup_steps)\n",
    "        \n",
    "        return lr\n",
    "    \n",
    "\n",
    "    return step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388df218",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c02f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEmbedding(Layer):\n",
    "    def __init__(self, units,dropout_rate,  **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.conv_1 = Conv1D(filters  = units, kernel_size = 1)\n",
    "        self.projection = Dense(units, kernel_initializer=TruncatedNormal(stddev=0.02))\n",
    "\n",
    "        self.dropout = Dropout(rate=dropout_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(PositionalEmbedding, self).build(input_shape)\n",
    "\n",
    "        self.position = self.add_weight(\n",
    "            name=\"position\",\n",
    "            shape=(1, input_shape[1], self.units),\n",
    "            initializer=TruncatedNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.projection(inputs)\n",
    "        # x = self.conv_1(inputs)\n",
    "        x = x + self.position\n",
    "        return self.dropout(x, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756111d",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e956fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(\n",
    "        self, embed_dim, mlp_dim, num_heads, dropout_rate,\n",
    "        attention_dropout_rate, **kwargs\n",
    "    ):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        #embed_dim = 128, \n",
    "        #mlp_dim = 256\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads = num_heads,\n",
    "            key_dim = embed_dim,\n",
    "            dropout = attention_dropout_rate, \n",
    "            kernel_initializer = TruncatedNormal(stddev = 0.02)\n",
    "        )\n",
    "\n",
    "        self. dense_0 = Dense(\n",
    "            units = mlp_dim, \n",
    "            activation = \"gelu\", \n",
    "            kernel_initializer = TruncatedNormal(stddev = 0.02)\n",
    "        )\n",
    "\n",
    "        self.dense_1 = Dense(\n",
    "            units = embed_dim, \n",
    "            kernel_initializer = TruncatedNormal(stddev = 0.02)\n",
    "        )\n",
    "\n",
    "        self.conv_0 = Conv1D(filters = 4 , kernel_size = 1, activation = 'relu')\n",
    "        self.conv_1 = Conv1D(filters  = embed_dim, kernel_size = 1)\n",
    "\n",
    "        self.dropout_0 = Dropout(rate = dropout_rate)\n",
    "        self.dropout_1 = Dropout(rate = dropout_rate)\n",
    "\n",
    "        self.norm_0 = LayerNormalization(epsilon = 1e-6)\n",
    "        self.norm_1 = LayerNormalization(epsilon = 1e-6)\n",
    "\n",
    "        self.add_0 = Add()\n",
    "        self.add_1 = Add()\n",
    "    \n",
    "    def call(self, inputs, training , mask):\n",
    "\n",
    "\n",
    "        x = self.norm_0(inputs)\n",
    "        x = self.mha(\n",
    "            query = x, \n",
    "            value = x, \n",
    "            key = x,\n",
    "            attention_mask = mask,\n",
    "            training = training\n",
    "        ) #[batch_size, sequence_length, embed_dim][8, 500, 3]\n",
    "        x = self.dropout_0(x, training= training)\n",
    "        x = self.add_0([x, inputs])\n",
    "\n",
    "        #MLP block \n",
    "        y = self.norm_1(x)\n",
    "        y = self.dense_0(y) #[batch_size , sequence_length, neurons]\n",
    "        y = self.dropout_1(y, training)\n",
    "        y = self.dense_1(y)#[batch_size , sequence_lenght, neurons]\n",
    "        y = self.dropout_1(y, training)\n",
    "        \n",
    "\n",
    "        return self.add_1([x, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e6abf",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af2665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        embed_dim,\n",
    "        mlp_dim,\n",
    "        num_heads,\n",
    "        num_classes,\n",
    "        dropout_rate,\n",
    "        attention_dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Transformer, self).__init__(**kwargs)\n",
    "\n",
    "        # Input (normalization of RAW measurements)\n",
    "        self.input_norm = Normalization()\n",
    "        \n",
    "        #Making Layer\n",
    "        self.masking_layer = Masking(mask_value = 0.0)\n",
    "\n",
    "        # Input\n",
    "        self.pos_embs = PositionalEmbedding(embed_dim, dropout_rate)\n",
    "\n",
    "        # Encoder\n",
    "        self.e_layers = [\n",
    "            Encoder(embed_dim, mlp_dim, num_heads, dropout_rate, attention_dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Output\n",
    "        self.norm = LayerNormalization(epsilon=1e-5)\n",
    "        self.pool = GlobalAveragePooling1D(data_format = 'channels_first')\n",
    "        self.dense_0 = Dense(mlp_dim, activation = 'relu')\n",
    "        self.final_layer = Dense(1, kernel_initializer=\"zeros\", activation = 'sigmoid')\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        expanded_input = tf.cast(tf.tile(tf.expand_dims(inputs, axis=-2), [1, 1, 500,1]), tf.float32)\n",
    "        self.masking_layer.build(expanded_input.shape)\n",
    "        mask = self.masking_layer.compute_mask(expanded_input)\n",
    "        x = self.input_norm(inputs) \n",
    "        x = self.pos_embs(x, training=training)\n",
    "        for layer in self.e_layers:\n",
    "            x = layer(x, training=training , mask = mask)\n",
    "        x = self.norm(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense_0(x)\n",
    "\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380a929",
   "metadata": {},
   "source": [
    "## Train Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8342ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config = {\n",
    "      'epochs': 50,\n",
    "      'num_layers':  3,\n",
    "      'embed_layer_size': 3,\n",
    "      'global_clipnorm' : 3.0,\n",
    "      'fc_layer_size': 256,\n",
    "      'num_heads': 2,\n",
    "      'dropout': 0.1,\n",
    "      'attention_dropout': 0.1,\n",
    "      'optimizer': 'adam',\n",
    "      'amsgrad': False,\n",
    "      'label_smoothing': 0.1,\n",
    "      'learning_rate': 1e-3,\n",
    "      #'weight_decay': {\n",
    "      #    'values': [2.5e-4, 1e-4, 5e-5, 1e-5]\n",
    "      'warmup_steps': 5,\n",
    "      'batch_size': 8}\n",
    "  \n",
    "\n",
    "    # config = wandb.config\n",
    "    \n",
    "    # Generate new model\n",
    "    model = Transformer(\n",
    "      num_layers=config['num_layers'],\n",
    "      embed_dim=config['embed_layer_size']\n",
    "      ,\n",
    "      mlp_dim=config['fc_layer_size'],\n",
    "      num_heads=config['num_heads'],\n",
    "      num_classes=2,\n",
    "      dropout_rate=config['dropout'],\n",
    "      attention_dropout_rate=config['attention_dropout'],\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # adapt on training dataset - must be before model.compile !!!\n",
    "    model.input_norm.adapt(X_train, batch_size=config['batch_size'])\n",
    "    # print(model.input_norm.variables)\n",
    "\n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == \"adam\":\n",
    "      optim = Adam(\n",
    "          global_clipnorm=config['global_clipnorm'],\n",
    "          amsgrad=config['amsgrad'],\n",
    "      )\n",
    "    # elif config.optimizer == \"adamw\":\n",
    "    #   optim = AdamW(\n",
    "    #       weight_decay=config.weight_decay,\n",
    "    #       amsgrad=config.amsgrad,\n",
    "    #       global_clipnorm=config.global_clipnorm,\n",
    "    #       exclude_from_weight_decay=[\"position\"]\n",
    "    #   )\n",
    "    else:\n",
    "      raise ValueError(\"The used optimizer is not in list of available\")\n",
    "\n",
    "    model.compile(\n",
    "      loss= BinaryCrossentropy(label_smoothing=config['label_smoothing']),\n",
    "      optimizer=optim,\n",
    "      metrics=[Recall(), Precision() , AUC(), F1Score(num_classes = 1)],\n",
    "    )\n",
    "    \n",
    "    checkpoint_filepath = os.path.join(os.getcwd(), 'tmp/chekcpoint')\n",
    "    model_checkpoint = ModelCheckpoint(filepath = checkpoint_filepath, \n",
    "                                      save_weights_only = True, \n",
    "                                      monitor = 'val_precision', \n",
    "                                      mode = 'max', \n",
    "                                      save_best_only = True, \n",
    "                                      verbose = True)\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "      X_train,\n",
    "      y_train,\n",
    "      batch_size=config['batch_size'],\n",
    "      epochs=config['epochs'],\n",
    "      validation_data=(X_val, y_val),\n",
    "      callbacks=[\n",
    "        LearningRateScheduler(cosine_schedule(base_lr=config['learning_rate'], total_steps=config['epochs'], warmup_steps=config['warmup_steps'])),\n",
    "        EarlyStopping(monitor=\"val_loss\", mode='min', min_delta=0.001, patience=5),\n",
    "        model_checkpoint\n",
    "      ],\n",
    "      verbose=1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f53c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Epoch 1/50\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.6927 - recall: 0.9003 - precision: 0.5581 - auc: 0.5191 - f1_score: 0.6741"
     ]
    }
   ],
   "source": [
    "    model = None\n",
    "    dataset_path = os.path.join(os.getcwd(), 'fall_detection_dataset.npz')\n",
    "    f = np.load(dataset_path)\n",
    "    signals = f['trials']\n",
    "\n",
    "    labels = f['labels']\n",
    "\n",
    "    # split to train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        signals, labels, test_size=0.15, random_state=9, stratify=labels\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.15, random_state=9, stratify=y_train\n",
    "    )\n",
    "\n",
    "    # sweep_id = wandb.sweep(sweep_config, project=\"KD_Transformer\")\n",
    "    with tf.device('/gpu:0'):\n",
    "      model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01c9868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 71ms/step - loss: 0.4293 - recall_6: 0.8841 - precision_6: 0.8133 - auc_6: 0.9266\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=8 , steps = X_test.shape[0] / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5b4d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = 2/((1/score[1]) + (1/score[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "974cdac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8472222344846358"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2898867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tousif/Lstm_transformer/KD_Multimodal/tmp/chekcpoint'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(), 'tmp/chekcpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4d41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
