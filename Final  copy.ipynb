{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9dde82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import re\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34d11393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlrd==1.1.0\n",
      "  Downloading xlrd-1.1.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: xlrd\n",
      "  Attempting uninstall: xlrd\n",
      "    Found existing installation: xlrd 1.0.0\n",
      "    Uninstalling xlrd-1.0.0:\n",
      "      Successfully uninstalled xlrd-1.0.0\n",
      "Successfully installed xlrd-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xlrd==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dad3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f344b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, Callback\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import precision_recall_curve\n",
    "from tensorflow.keras.layers import Add, Dense, LayerNormalization, Masking, GlobalAveragePooling1D, Conv1D, Dropout, MultiHeadAttention, Layer\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, Callback, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.metrics import Recall, Precision , AUC\n",
    "#from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "#from sklearn.metrics import auc, roc_curve\n",
    "#import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e3df7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(mode = 'train'):\n",
    "    new_len = 256\n",
    "    in_chan = 3\n",
    "    fall = os.path.join(os.getcwd() , f'dataset_Alex/{mode}/Fall/***.xlsx')\n",
    "    adl = os.path.join(os.getcwd(), f'dataset_Alex/{mode}/ADL/***.xlsx')\n",
    "    fall_files = glob.glob(fall)\n",
    "    adl_files = glob.glob(adl)\n",
    "\n",
    "    all_file_path = fall_files + adl_files\n",
    "    samples = len(all_file_path)\n",
    "    dataset = np.zeros((1,new_len, in_chan))\n",
    "    \n",
    "    if len(all_file_path) == 0:\n",
    "        raise Exception('Data files not found')\n",
    " \n",
    "    trials_count = {}\n",
    "    fall_pattern = re.compile(\"Fall\")\n",
    "    trials = []\n",
    "    labels = []\n",
    "    length = []\n",
    "    #count = 0\n",
    "\n",
    "    for file_path in all_file_path:\n",
    "        label = None\n",
    "        if fall_pattern.search(file_path):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        #checking if the excel has 2 sheets or not\n",
    "        if len (pd.ExcelFile(file_path).sheet_names) == 2:\n",
    "                df = pd.read_excel(file_path, sheet_name=-1)\n",
    "                df = df.iloc[:, :6]\n",
    "                null_col = df[df.isnull().any(axis = 1)].index.to_list()\n",
    "                \n",
    "                if len(null_col) % 10 != 0  :\n",
    "                    print(f'{file_path} trimmed file contains {len(null_col)} of null rows')\n",
    "                    continue\n",
    "                    \n",
    "                    #raise Exception(f'{file_path} trimmed file contains {len(null_col)} of null rows')\n",
    "                #calculating how many null segments we have \n",
    "                null_col = df[df.isnull().any(axis = 1)].index.to_list()\n",
    "                null_seg = len(null_col)//10\n",
    "                \n",
    "\n",
    "                trial_start_lst = null_col[9::10]\n",
    "                trial_end_lst = null_col[10::10]\n",
    "                \n",
    "                for i in range(len(null_col)//10 + 1):\n",
    "                    #trials_count[label] = trials_count.get(label , 0) + 1\n",
    "                    trial = None\n",
    "                    if i == 0 :\n",
    "\n",
    "                        trial = df.iloc[0:null_col[1]-1, 3:6]\n",
    "                    elif i == null_seg :\n",
    "                        trial = df.iloc[trial_start_lst[-1]+1:, 3:6]\n",
    "                    else: \n",
    "                        trial_end = trial_end_lst[i-1]\n",
    "                        trial_start = trial_start_lst[i-1] + 1\n",
    "                        trial = df.iloc[trial_start : trial_end-1 , 3:6]\n",
    "                        trial.dropna(inplace = True)\n",
    "                    \n",
    "                    #trial = tf.convert_to_tensor(trial.values, dtype = tf.float32)\n",
    "                    act_len = trial.shape[0]\n",
    "                    channel = trial.shape[1]\n",
    "                    x = np.linspace(1, act_len, num=new_len)\n",
    "                    xp = np.linspace(1, act_len, num= act_len)\n",
    "                    interpolated_data = np.zeros((1,new_len,channel))\n",
    "                    trial = trial.to_numpy()\n",
    "                    if trial.shape[0] == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    else:\n",
    "                        for idx in range(channel):\n",
    "                            yp = trial[:, idx]\n",
    "                            axis_interpolated = np.interp(x, xp, yp)\n",
    "                            interpolated_data[0,:, idx] = axis_interpolated\n",
    "                        dataset = np.concatenate((dataset,interpolated_data), axis = 0)\n",
    "                        labels.append(label)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(f'{file_path} doesnt have trimmed data')\n",
    "        \n",
    "    return tf.convert_to_tensor(dataset[1:, :, :]), tf.convert_to_tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8464755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_preprocessing(mode = 'train'):\n",
    "    window_length = 50\n",
    "    step_size = 25\n",
    "    in_chan = 3\n",
    "    #finding the path to all fall and adl data\n",
    "    fall = os.path.join(os.getcwd() , f'dataset_Alex/{mode}/Fall/***.xlsx')\n",
    "    adl = os.path.join(os.getcwd(), f'dataset_Alex/{mode}/ADL/***.xlsx')\n",
    "    fall_files = glob.glob(fall)\n",
    "    adl_files = glob.glob(adl)\n",
    "\n",
    "    all_file_path = fall_files + adl_files\n",
    "    samples = len(all_file_path)\n",
    "    \n",
    "    #initiating the dataset array\n",
    "    dataset = np.zeros((1,new_len, in_chan))\n",
    "    \n",
    "    if len(all_file_path) == 0:\n",
    "        raise Exception('Data files not found')\n",
    " \n",
    "    trials_count = {}\n",
    "    fall_pattern = re.compile(\"Fall\")\n",
    "    trials = []\n",
    "    labels = []\n",
    "    length = []\n",
    "    #count = 0\n",
    "\n",
    "    for file_path in all_file_path:\n",
    "        label = None\n",
    "        if fall_pattern.search(file_path):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "            step_size *= 2\n",
    "        \n",
    "            \n",
    "\n",
    "        #checking if the excel has 2 sheets or not\n",
    "        if len (pd.ExcelFile(file_path).sheet_names) == 2:\n",
    "                df = pd.read_excel(file_path, sheet_name=-1)\n",
    "                df = df.iloc[:, :6]\n",
    "                null_col = df[df.isnull().any(axis = 1)].index.to_list()\n",
    "                \n",
    "                if len(null_col) % 10 != 0  :\n",
    "                    raise Exception(f'{file_path} trimmed file contains {len(null_col)} of null rows')\n",
    "                #calculating how many null segments we have \n",
    "                null_col = df[df.isnull().any(axis = 1)].index.to_list()\n",
    "                null_seg = len(null_col)//10\n",
    "                \n",
    "\n",
    "                trial_start_lst = null_col[9::10]\n",
    "                trial_end_lst = null_col[10::10]\n",
    "                \n",
    "                for i in range(len(null_col)//10 + 1):\n",
    "                    #trials_count[label] = trials_count.get(label , 0) + 1\n",
    "                    trial = None\n",
    "                    if i == 0 :\n",
    "\n",
    "                        trial = df.iloc[0:null_col[1]-1, 3:6]\n",
    "                    elif i == null_seg :\n",
    "                        trial = df.iloc[trial_start_lst[-1]+1:, 3:6]\n",
    "                    else: \n",
    "                        trial_end = trial_end_lst[i-1]\n",
    "                        trial_start = trial_start_lst[i-1] + 1\n",
    "                        trial = df.iloc[trial_start : trial_end-1 , 3:6]\n",
    "                        trial.dropna(inplace = True)\n",
    "                    \n",
    "                    #trial = tf.convert_to_tensor(trial.values, dtype = tf.float32)\n",
    "                    act_len = trial.shape[0]\n",
    "                    channel = trial.shape[1]\n",
    "                    x = np.linspace(1, act_len, num=new_len)\n",
    "                    xp = np.linspace(1, act_len, num= act_len)\n",
    "                    interpolated_data = np.zeros((1,new_len,channel))\n",
    "                    trial = trial.to_numpy()\n",
    "                    if trial.shape[0] == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    else:\n",
    "                        for idx in range(channel):\n",
    "                            yp = trial[:, idx]\n",
    "                            axis_interpolated = np.interp(x, xp, yp)\n",
    "                            interpolated_data[0,:, idx] = axis_interpolated\n",
    "                        dataset = np.concatenate((dataset,interpolated_data), axis = 0)\n",
    "                        labels.append(label)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(f'{file_path} doesnt have trimmed data')\n",
    "        \n",
    "    return tf.convert_to_tensor(dataset[1:, :, :]), tf.convert_to_tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77847252",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8bf8872f4173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#saving the dataset to np file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#np.savez('alex_val', trials= dataset , labels = labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4188fc232e41>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{file_path} doesnt have trimmed data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1403\u001b[0m   \"\"\"\n\u001b[1;32m   1404\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1405\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_tfrt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m           \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ContextOptionsSetTfrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_tfrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mcontext_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_NewContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_DeleteContextOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "#saving the dataset to np file\n",
    "dataset , labels = preprocessing(mode = \"train\")\n",
    "#np.savez('alex_val', trials= dataset , labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0ba524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1_Score(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.f1 = self.add_weight(name='f1', initializer='zeros')\n",
    "        self.precision_fn = Precision(thresholds=0.5)\n",
    "        self.recall_fn = Recall(thresholds=0.5)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        p = self.precision_fn(y_true, y_pred)\n",
    "        r = self.recall_fn(y_true, y_pred)\n",
    "        # since f1 is a variable, we use assign\n",
    "        self.f1.assign(2 * ((p * r) / (p + r + 1e-6)))\n",
    "\n",
    "    def result(self):\n",
    "        return self.f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        # we also need to reset the state of the precision and recall objects\n",
    "        self.precision_fn.reset_states()\n",
    "        self.recall_fn.reset_states()\n",
    "        self.f1.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83288736",
   "metadata": {},
   "source": [
    "## Lr Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9b8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "def cosine_schedule(base_lr, total_steps, warmup_steps ):\n",
    "    def step_fn(epoch):\n",
    "        lr = base_lr \n",
    "        progress = (epoch - warmup_steps) / float(total_steps -  warmup_steps)\n",
    "\n",
    "        progress = tf.clip_by_value(progress, 0.0, 1.0)\n",
    "\n",
    "        lr = lr * 0.5 * (1.0 + tf.cos(math.pi * progress))\n",
    "        \n",
    "        if warmup_steps:\n",
    "            lr = lr * tf.minimum(1.0 , epoch/warmup_steps)\n",
    "        \n",
    "        return lr\n",
    "    \n",
    "\n",
    "    return step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388df218",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c02f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEmbedding(Layer):\n",
    "    def __init__(self, units,dropout_rate,  **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.conv_1 = Conv1D(filters  = units, kernel_size = 1)\n",
    "        self.projection = Dense(units, kernel_initializer=TruncatedNormal(stddev=0.02))\n",
    "\n",
    "        self.dropout = Dropout(rate=dropout_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(PositionalEmbedding, self).build(input_shape)\n",
    "\n",
    "        self.position = self.add_weight(\n",
    "            name=\"position\",\n",
    "            shape=(1, input_shape[1], self.units),\n",
    "            initializer=TruncatedNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.projection(inputs)\n",
    "        # x = self.conv_1(inputs)\n",
    "        x = x + self.position\n",
    "        return self.dropout(x, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756111d",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e956fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(\n",
    "        self, embed_dim, mlp_dim, num_heads, dropout_rate,\n",
    "        attention_dropout_rate, **kwargs\n",
    "    ):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads = num_heads,\n",
    "            key_dim = embed_dim,\n",
    "            dropout = attention_dropout_rate, \n",
    "            kernel_initializer = TruncatedNormal(stddev = 0.02)\n",
    "        )\n",
    "\n",
    "        self. dense_0 = Dense(\n",
    "            units = mlp_dim, \n",
    "            activation = \"gelu\", \n",
    "            kernel_initializer = TruncatedNormal(stddev = 0.02)\n",
    "        )\n",
    "\n",
    "        self.dense_1 = Dense(\n",
    "            units = embed_dim, \n",
    "            kernel_initializer = TruncatedNormal(stddev = 0.02)\n",
    "        )\n",
    "\n",
    "        self.conv_0 = Conv1D(filters = 4 , kernel_size = 1, activation = 'relu')\n",
    "        self.conv_1 = Conv1D(filters  = embed_dim, kernel_size = 1)\n",
    "\n",
    "        self.dropout_0 = Dropout(rate = dropout_rate)\n",
    "        self.dropout_1 = Dropout(rate = dropout_rate)\n",
    "\n",
    "        self.norm_0 = LayerNormalization(epsilon = 1e-6)\n",
    "        self.norm_1 = LayerNormalization(epsilon = 1e-6)\n",
    "\n",
    "        self.add_0 = Add()\n",
    "        self.add_1 = Add()\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "\n",
    "\n",
    "        x = self.norm_0(inputs)\n",
    "        x = self.mha(\n",
    "            query = x, \n",
    "            value = x, \n",
    "            key = x,\n",
    "            training = training\n",
    "        ) #[batch_size, sequence_length, embed_dim][8, 500, 3]\n",
    "        x = self.dropout_0(x, training= training)\n",
    "        x = self.add_0([x, inputs])\n",
    "\n",
    "        #MLP block \n",
    "        y = self.norm_1(x)\n",
    "        y = self.dense_0(y) #[batch_size , sequence_length, neurons]\n",
    "        y = self.dropout_1(y, training)\n",
    "        y = self.dense_1(y)#[batch_size , sequence_lenght, neurons]\n",
    "        y = self.dropout_1(y, training)\n",
    "        \n",
    "\n",
    "        return self.add_1([x, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e6abf",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6af2665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        embed_dim,\n",
    "        mlp_dim,\n",
    "        num_heads,\n",
    "        num_classes,\n",
    "        dropout_rate,\n",
    "        attention_dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Transformer, self).__init__(**kwargs)\n",
    "\n",
    "        # Input (normalization of RAW measurements)\n",
    "        self.input_norm = Normalization()\n",
    "        \n",
    "\n",
    "        # Input\n",
    "        self.pos_embs = PositionalEmbedding(embed_dim, dropout_rate)\n",
    "\n",
    "        # Encoder\n",
    "        self.e_layers = [\n",
    "            Encoder(embed_dim, mlp_dim, num_heads, dropout_rate, attention_dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Output\n",
    "        self.norm = LayerNormalization(epsilon=1e-5)\n",
    "        self.pool = GlobalAveragePooling1D(data_format = 'channels_first')\n",
    "        self.dense_0 = Dense(mlp_dim, activation = 'relu')\n",
    "        self.final_layer = Dense(1, kernel_initializer=\"zeros\", activation = 'sigmoid')\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        #expanded_input = tf.cast(tf.tile(tf.expand_dims(inputs, axis=-2), [1, 1, 500,1]), tf.float32)\n",
    "        #self.masking_layer.build(expanded_input.shape)\n",
    "        #mask = self.masking_layer.compute_mask(expanded_input)\n",
    "        x = self.input_norm(inputs) \n",
    "        x = self.pos_embs(x, training=training)\n",
    "        for layer in self.e_layers:\n",
    "            x = layer(x, training=training)\n",
    "        x = self.norm(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense_0(x)\n",
    "\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380a929",
   "metadata": {},
   "source": [
    "## Train Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6cc2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    #model.input_norm.adapt(X_train, batch_size=config['batch_size'])\n",
    "    config = {\n",
    "      'epochs': 50,\n",
    "      'num_layers':  2,\n",
    "      'embed_layer_size': 3,\n",
    "      'global_clipnorm' : 3.0,\n",
    "      'fc_layer_size': 256,\n",
    "      'num_heads': 4,\n",
    "      'dropout': 0.1,\n",
    "      'attention_dropout': 0.1,\n",
    "      'optimizer': 'adam',\n",
    "      'amsgrad': False,\n",
    "      'label_smoothing': 0.1,\n",
    "      'learning_rate': 1e-3,\n",
    "      #'weight_decay': {\n",
    "      #    'values': [2.5e-4, 1e-4, 5e-5, 1e-5]\n",
    "      'warmup_steps': 5,\n",
    "      'batch_size': 8}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428f63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def create_model(data):\n",
    "\n",
    "\n",
    "    # config = wandb.config\n",
    "    \n",
    "    # Generate new model\n",
    "    model = Transformer(\n",
    "      num_layers=config['num_layers'],\n",
    "      embed_dim=config['embed_layer_size'],\n",
    "      mlp_dim=config['fc_layer_size'],\n",
    "      num_heads=config['num_heads'],\n",
    "      num_classes=2,\n",
    "      dropout_rate=config['dropout'],\n",
    "      attention_dropout_rate=config['attention_dropout'],\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # adapt on training dataset - must be before model.compile !!!\n",
    "    model.input_norm.adapt(data)\n",
    "\n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == \"adam\":\n",
    "      optim = Adam(\n",
    "          global_clipnorm=config['global_clipnorm'],\n",
    "          amsgrad=config['amsgrad'],\n",
    "      )\n",
    "    # elif config.optimizer == \"adamw\":\n",
    "    #   optim = AdamW(\n",
    "    #       weight_decay=config.weight_decay,\n",
    "    #       amsgrad=config.amsgrad,\n",
    "    #       global_clipnorm=config.global_clipnorm,\n",
    "    #       exclude_from_weight_decay=[\"position\"]\n",
    "    #   )\n",
    "    else:\n",
    "      raise ValueError(\"The used optimizer is not in list of available\")\n",
    "\n",
    "    model.compile(\n",
    "      loss= BinaryCrossentropy(label_smoothing=config['label_smoothing']),\n",
    "      optimizer=optim,\n",
    "      metrics=[Recall(), Precision() , AUC(), F1_Score()],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8342ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = create_model(data = X_train)\n",
    "    checkpoint_filepath = os.path.join(os.getcwd(), 'tmp/weights.ckpt')\n",
    "    model_checkpoint = ModelCheckpoint(filepath = checkpoint_filepath, \n",
    "                                      save_weights_only = True, \n",
    "                                      monitor = 'val_f1_score', \n",
    "                                      mode = 'max', \n",
    "                                      save_best_only = True, \n",
    "                                      verbose = True)\n",
    "\n",
    "    log_dir = \"logs/\"  # Specify the directory where TensorBoard logs will be saved\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    # print(model.input_norm.variables)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "      X_train,\n",
    "      y_train,\n",
    "      batch_size=config['batch_size'],\n",
    "      epochs=config['epochs'],\n",
    "      validation_data=(X_val, y_val),\n",
    "      shuffle = True,\n",
    "      callbacks=[\n",
    "        LearningRateScheduler(cosine_schedule(base_lr=config['learning_rate'], total_steps=config['epochs'], warmup_steps=config['warmup_steps'])),\n",
    "        #EarlyStopping(monitor=\"loss\", mode='min', min_delta=0.001, patience=5),\n",
    "        model_checkpoint, tensorboard_callback\n",
    "      ],\n",
    "      verbose=1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06f53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = None\n",
    "    train_dataset_path = os.path.join(os.getcwd(), 'alex_train.npz')\n",
    "    val_dataset_path = os.path.join(os.getcwd(), 'alex_val.npz')\n",
    "    #loading train dataset\n",
    "    train_data = np.load(train_dataset_path)\n",
    "    X_train = train_data['trials']\n",
    "    y_train = train_data['labels']\n",
    "    \n",
    "    #loading val dataset \n",
    "    val_data = np.load(val_dataset_path)\n",
    "    X_val = val_data['trials']\n",
    "    y_val = val_data['labels']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9bfaf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-879a4039e135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b4bcc08edbe2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       ],\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "      model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8e9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_dataset_path = os.path.join(os.getcwd(), 'alex_test.npz')\n",
    "test_data = np.load(test_dataset_path)\n",
    "\n",
    "X_test = test_data['trials']\n",
    "y_test = test_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9948c73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 256, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c16bbb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(X_test).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf348a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b2cc6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.window(1, shift = 10, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ea7bbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 256, 3)\n",
      "(8, 256, 3)\n",
      "(8, 256, 3)\n",
      "(8, 256, 3)\n",
      "(8, 256, 3)\n",
      "(8, 256, 3)\n",
      "(7, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "for elements in dataset:\n",
    "    print(elements.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d281ab",
   "metadata": {},
   "source": [
    "### Loading the Best Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b4d41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 19ms/step - loss: 0.6931 - recall_2: 0.0000e+00 - precision_2: 0.0000e+00 - auc_1: 0.3750 - f1_score: 0.0000e+00\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6633 - recall_2: 0.8000 - precision_2: 0.5333 - auc_1: 0.7093 - f1_score: 0.6400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6632546782493591,\n",
       " 0.800000011920929,\n",
       " 0.5333333611488342,\n",
       " 0.7092856764793396,\n",
       " 0.639999508857727]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(data=X_train)\n",
    "model.evaluate(X_test, y_test, batch_size=8, steps=X_test.shape[0]/8)\n",
    "\n",
    "#loading best weights and evaluating\n",
    "weight_path = 'tmp/weights.ckpt'\n",
    "model.load_weights(weight_path)\n",
    "model.evaluate(X_test, y_test, batch_size=8, steps=X_test.shape[0]/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7dd7021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_2 (Normalizati multiple                  7         \n",
      "_________________________________________________________________\n",
      "positional_embedding_2 (Posi multiple                  780       \n",
      "_________________________________________________________________\n",
      "encoder_4 (Encoder)          multiple                  1990      \n",
      "_________________________________________________________________\n",
      "encoder_5 (Encoder)          multiple                  1990      \n",
      "_________________________________________________________________\n",
      "layer_normalization_14 (Laye multiple                  6         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             multiple                  257       \n",
      "=================================================================\n",
      "Total params: 70,822\n",
      "Trainable params: 70,815\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c527f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.compat.v1.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ae1f536f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.preprocessing.normalization.Normalization at 0x7f8ea469ce80>,\n",
       " <__main__.PositionalEmbedding at 0x7f8efc363da0>,\n",
       " <__main__.Encoder at 0x7f8ea469df60>,\n",
       " <__main__.Encoder at 0x7f8ea469df98>,\n",
       " <tensorflow.python.keras.layers.normalization.LayerNormalization at 0x7f8ea46b83c8>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling1D at 0x7f8ea47e8320>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f8ea46b8780>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f8ea46b8b00>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78646dc8",
   "metadata": {},
   "source": [
    "## Ploting ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc88dd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 336ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).ravel()\n",
    "fp_pred, tp_pred, threshold_pred = roc_curve(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c72ee3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8889392 , 0.16474193, 0.99482226, 0.9363375 , 0.95260257,\n",
       "       0.7315567 , 0.96716213, 0.71308637, 0.9427625 , 0.7331089 ,\n",
       "       0.53011787, 0.98499864, 0.99883133, 0.91073406, 0.82420546,\n",
       "       0.8478556 , 0.9957637 , 0.98465466, 0.99582255, 0.99457103,\n",
       "       0.76983017, 0.7252614 , 0.48596117, 0.41546848, 0.19717704,\n",
       "       0.0412966 , 0.21211296, 0.6689365 , 0.37978172, 0.4929512 ,\n",
       "       0.9272632 , 0.86242265, 0.92437226, 0.95877945, 0.7537029 ,\n",
       "       0.9250761 , 0.11383366, 0.07067526, 0.10001323, 0.33242407,\n",
       "       0.05488832, 0.07872478, 0.1397683 , 0.06928135, 0.2589604 ,\n",
       "       0.03677565, 0.04694097, 0.03518001, 0.06578131, 0.04774711,\n",
       "       0.06822002, 0.04714935, 0.03687645, 0.4182752 , 0.13995308],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d297b0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arry.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "575c44bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231174bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = auc(fp_pred, tp_pred)\n",
    "#pr = precision_recall_curve(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba6fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b598cd2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [82], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#plt.figure()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#plt.plot([0,1], [0, 1], 'k--')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#plt.plot(fp_pred, tp_pred, label = 'SmartFall (area = {:.3f})'.format(auc))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#plt.title('PR Curve')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#plt.show()\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_recall_curve\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m     14\u001b[0m precision, recall, thresholds \u001b[38;5;241m=\u001b[39m precision_recall_curve(y_test, y_pred)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#fpr, tpr, _ = roc_curve(y_real, y_pred)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "#plt.figure()\n",
    "#plt.plot([0,1], [0, 1], 'k--')\n",
    "#plt.plot(fp_pred, tp_pred, label = 'SmartFall (area = {:.3f})'.format(auc))\n",
    "#plt.xlabel('False Positive Rate')\n",
    "#plt.ylabel('True Positive Rate')\n",
    "#plt.title('PR Curve')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import plotly.express as px\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "    #fpr, tpr, _ = roc_curve(y_real, y_pred)\n",
    "    \n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "title = 'Precision-Recall Curve\\nauc: {:.4f}'.format(auc)\n",
    "plt.title(title)\n",
    "#title=f\"{title} (AUC={auc(false_positive_rate, true_positive_rate):.4f})\",\n",
    "plt.fill_between(recall, precision, alpha=0.2)\n",
    "plt.xlim([0.0, 1])\n",
    "plt.xticks([i * 0.5 for i in range(3)])\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = -x + 1\n",
    "plt.plot(x, y, color='r', linestyle='--', label='y = -x')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249246a5",
   "metadata": {},
   "source": [
    "### Creating Tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73875b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model, 'transformer', overwrite=True, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81701a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bgu9/KD_Multimodal/transformer\n"
     ]
    }
   ],
   "source": [
    "cd transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9c9d26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34massets\u001b[0m/  saved_model.pb  \u001b[01;34mvariables\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28c7b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4059d20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_7_layer_call_and_return_conditional_losses, dense_7_layer_call_fn, dropout_5_layer_call_and_return_conditional_losses, dropout_5_layer_call_fn, dense_7_layer_call_fn while saving (showing 5 of 160). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as dense_7_layer_call_and_return_conditional_losses, dense_7_layer_call_fn, dropout_5_layer_call_and_return_conditional_losses, dropout_5_layer_call_fn, dense_7_layer_call_fn while saving (showing 5 of 160). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops. <-- Add this line\n",
    "]\n",
    "converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.allow_custom_ops=True\n",
    "converter.experimental_new_converter =True\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f342eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e96b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "372baf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c43635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_test[1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90633779",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22463369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f598f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set input tensor to the interpreter\n",
    "interpreter.set_tensor(input_details[0]['index'], data.astype(np.float32))\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor and post-process the results (example)\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Inference result:\", output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca9aff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization multiple                  7         \n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You tried to call `count_params` on masking, but the layer isn't built. You can build it manually via: `masking.build(batch_input_shape)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2382\u001b[0m                               \u001b[0mline_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m                               \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2384\u001b[0;31m                               print_fn=print_fn)\n\u001b[0m\u001b[1;32m   2385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2386\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/keras/utils/layer_utils.py\u001b[0m in \u001b[0;36mprint_summary\u001b[0;34m(model, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequential_like\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m       \u001b[0mprint_layer_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0mprint_layer_summary_with_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/keras/utils/layer_utils.py\u001b[0m in \u001b[0;36mprint_layer_summary\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mcls_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' ('\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mprint_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfold/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mcount_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2206\u001b[0m                          \u001b[0;34m', but the layer isn\\'t built. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m                          \u001b[0;34m'You can build it manually via: `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2208\u001b[0;31m                          '.build(batch_input_shape)`.')\n\u001b[0m\u001b[1;32m   2209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You tried to call `count_params` on masking, but the layer isn't built. You can build it manually via: `masking.build(batch_input_shape)`."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
