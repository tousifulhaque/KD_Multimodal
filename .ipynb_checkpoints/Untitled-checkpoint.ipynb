{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdede410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Creating Dataset successful\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import re\n",
    "import statistics\n",
    "\n",
    "\n",
    "def preprocessing(mode = 'train'):\n",
    "    \n",
    "    # storing all file paths in  all_file_path\n",
    "    fall = os.path.join(os.getcwd() , f'datasets/{mode}/fall/***.xlsx')\n",
    "    adl = os.path.join(os.getcwd(), f'datasets/{mode}/adl/***.xlsx')\n",
    "    fall_files = glob.glob(fall)\n",
    "    adl_files = glob.glob(adl)\n",
    "    all_file_path = fall_files + adl_files\n",
    "\n",
    "    trials_count = {}\n",
    "    fall_pattern = re.compile(\"Fall\")\n",
    "    trials = []\n",
    "    labels = []\n",
    "    length = []\n",
    "    fall_count = 0\n",
    "    \n",
    "    \n",
    "    for file_path in all_file_path:\n",
    "        label = None\n",
    "        \n",
    "        #if the file name have Fall in it then the label is 1.\n",
    "        if fall_pattern.search(file_path):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        #checking if the excel has 2 sheets or not\n",
    "        if len (pd.ExcelFile(file_path).sheet_names) == 2:\n",
    "                df = pd.read_excel(file_path, sheet_name=-1)\n",
    "                df = df.iloc[:, :6]\n",
    "                null_col = df[df.isnull().any(axis = 1)].index.to_list()\n",
    "                \n",
    "                if len(null_col) % 10 != 0  :\n",
    "                    raise Exception(f'{filepath} trimmed file contains {len(null_col)} of null rows')\n",
    "                    \n",
    "                #calculating how many null segments we have \n",
    "                null_col = df[df.isnull().any(axis = 1)].index.to_list()\n",
    "                null_seg = len(null_col)//10\n",
    "                \n",
    "                #adding the start and end point of a trial\n",
    "                trial_start_lst = null_col[9::10]\n",
    "                trial_end_lst = null_col[10::10]\n",
    "                \n",
    "                for i in range(len(null_col)//10 + 1):\n",
    "                    trials_count[label] = trials_count.get(label , 0) + 1\n",
    "                    trial = None\n",
    "                    \n",
    "                    #don't have the intial timestamp of trial 1 and the end timestamp of last trial\n",
    "                    if i == 0 :\n",
    "                        trial = df.iloc[0:null_col[1]-1, 3:6]\n",
    "                    elif i == null_seg :\n",
    "                        trial = df.iloc[trial_start_lst[-1]+1:, 3:6]\n",
    "                    else: \n",
    "                        trial_end = trial_end_lst[i-1]\n",
    "                        trial_start = trial_start_lst[i-1] + 1\n",
    "                        trial = df.iloc[trial_start : trial_end-1 , 3:6]\n",
    "                        trial.dropna(inplace = True)\n",
    "                                    \n",
    "                    trial = tf.convert_to_tensor(trial.values, dtype = tf.float32)\n",
    "                    if trial.shape[0] > 300:\n",
    "                        # print(file_path)\n",
    "                        length.append(trial.shape[0])\n",
    "                    \n",
    "                    labels.append(label)\n",
    "                    trials.append(trial)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(f'{file_path} doesnt have trimmed data')\n",
    "\n",
    " \n",
    "    # # print(f'Min {min(length)} , Median {statistics.median(length)}, Max {max(length)}, Mean {statistics.mean(length)}')\n",
    "    # trials = tf.keras.utils.pad_sequences(trials, maxlen= 300, value = 0.0 , dtype = float, padding = 'post' )\n",
    "    \n",
    "\n",
    "\n",
    "    # #transposing the trials \n",
    "    # trials = tf.transpose(trials, perm = [0,2,1])\n",
    "    print(trials_count)\n",
    "\n",
    "\n",
    "    try:\n",
    "         np.savez_compressed(\"fall_detection_dataset\", trials=trials, labels=labels)\n",
    "         print('Dataset creation successful')\n",
    "    except:\n",
    "        raise RuntimeError(\"Failed creating the dataset\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
