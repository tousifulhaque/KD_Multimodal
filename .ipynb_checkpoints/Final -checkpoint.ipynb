{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9dde82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dad3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f344b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  wandb_config import sweep_config\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow_addons.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, Callback\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Add, Dense, LayerNormalization, Normalization , Masking, GlobalAveragePooling1D, Conv1D, Dropout, MultiHeadAttention, Layer\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, Callback, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.metrics import Recall, Precision , AUC\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42faf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1_Score(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.f1 = self.add_weight(name='f1', initializer='zeros')\n",
    "        self.precision_fn = Precision(thresholds=0.5)\n",
    "        self.recall_fn = Recall(thresholds=0.5)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        p = self.precision_fn(y_true, y_pred)\n",
    "        r = self.recall_fn(y_true, y_pred)\n",
    "        # since f1 is a variable, we use assign\n",
    "        self.f1.assign(2 * ((p * r) / (p + r + 1e-6)))\n",
    "\n",
    "    def result(self):\n",
    "        return self.f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        # we also need to reset the state of the precision and recall objects\n",
    "        self.precision_fn.reset_states()\n",
    "        self.recall_fn.reset_states()\n",
    "        self.f1.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83288736",
   "metadata": {},
   "source": [
    "## Lr Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff9b8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "# from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# class PrintLR(Callback):\n",
    "#     def on_epoch_end(self, epoch, logs = None):\n",
    "\n",
    "\n",
    "def cosine_schedule(base_lr, total_steps, warmup_steps ):\n",
    "    def step_fn(epoch):\n",
    "        lr = base_lr\n",
    "        epoch = 1\n",
    "\n",
    "        progress = (epoch - warmup_steps) / float(total_steps -  warmup_steps)\n",
    "\n",
    "        progress = tf.clip_by_value(progress, 0.0, 1.0)\n",
    "\n",
    "        lr = lr * 0.5 * (1.0 + tf.cos(math.pi * progress))\n",
    "        \n",
    "        if warmup_steps:\n",
    "            lr = lr * tf.minimum(1.0 , epoch/warmup_steps)\n",
    "        \n",
    "        return lr\n",
    "    \n",
    "\n",
    "    return step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388df218",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c02f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEmbedding(Layer):\n",
    "    def __init__(self, units,dropout_rate,  **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.conv_1 = Conv1D(filters  = units, kernel_size = 1)\n",
    "        self.projection = Dense(units, kernel_initializer=TruncatedNormal(stddev=0.02))\n",
    "\n",
    "        self.dropout = Dropout(rate=dropout_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(PositionalEmbedding, self).build(input_shape)\n",
    "\n",
    "        self.position = self.add_weight(\n",
    "            name=\"position\",\n",
    "            shape=(1, input_shape[1], self.units),\n",
    "            initializer=TruncatedNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.projection(inputs)\n",
    "        # x = self.conv_1(inputs)\n",
    "        x = x + self.position\n",
    "        return self.dropout(x, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756111d",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e956fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(\n",
    "        self, embed_dim, mlp_dim, num_heads, dropout_rate,\n",
    "        attention_dropout_rate, **kwargs\n",
    "    ):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        #embed_dim = 128, \n",
    "        #mlp_dim = 256\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads = num_heads,\n",
    "            key_dim = embed_dim,\n",
    "            dropout = attention_dropout_rate, \n",
    "            kernel_initializer = TruncatedNormal(stddev = 0.02)\n",
    "        )\n",
    "\n",
    "        self. dense_0 = Dense(\n",
    "            units = mlp_dim, \n",
    "            activation = \"gelu\", \n",
    "            kernel_initializer = TruncatedNormal(stddev = 0.02)\n",
    "        )\n",
    "\n",
    "        self.dense_1 = Dense(\n",
    "            units = embed_dim, \n",
    "            kernel_initializer = TruncatedNormal(stddev = 0.02)\n",
    "        )\n",
    "\n",
    "        self.conv_0 = Conv1D(filters = 4 , kernel_size = 1, activation = 'relu')\n",
    "        self.conv_1 = Conv1D(filters  = embed_dim, kernel_size = 1)\n",
    "\n",
    "        self.dropout_0 = Dropout(rate = dropout_rate)\n",
    "        self.dropout_1 = Dropout(rate = dropout_rate)\n",
    "\n",
    "        self.norm_0 = LayerNormalization(epsilon = 1e-6)\n",
    "        self.norm_1 = LayerNormalization(epsilon = 1e-6)\n",
    "\n",
    "        self.add_0 = Add()\n",
    "        self.add_1 = Add()\n",
    "    \n",
    "    def call(self, inputs, training , mask):\n",
    "\n",
    "\n",
    "        x = self.norm_0(inputs)\n",
    "        x = self.mha(\n",
    "            query = x, \n",
    "            value = x, \n",
    "            key = x,\n",
    "            attention_mask = mask,\n",
    "            training = training\n",
    "        ) #[batch_size, sequence_length, embed_dim][8, 500, 3]\n",
    "        x = self.dropout_0(x, training= training)\n",
    "        x = self.add_0([x, inputs])\n",
    "\n",
    "        #MLP block \n",
    "        y = self.norm_1(x)\n",
    "        y = self.dense_0(y) #[batch_size , sequence_length, neurons]\n",
    "        y = self.dropout_1(y, training)\n",
    "        y = self.dense_1(y)#[batch_size , sequence_lenght, neurons]\n",
    "        y = self.dropout_1(y, training)\n",
    "        \n",
    "\n",
    "        return self.add_1([x, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e6abf",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6af2665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        embed_dim,\n",
    "        mlp_dim,\n",
    "        num_heads,\n",
    "        num_classes,\n",
    "        dropout_rate,\n",
    "        attention_dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Transformer, self).__init__(**kwargs)\n",
    "\n",
    "        # Input (normalization of RAW measurements)\n",
    "        self.input_norm = Normalization()\n",
    "        \n",
    "        #Making Layer\n",
    "        self.masking_layer = Masking(mask_value = 0.0)\n",
    "\n",
    "        # Input\n",
    "        self.pos_embs = PositionalEmbedding(embed_dim, dropout_rate)\n",
    "\n",
    "        # Encoder\n",
    "        self.e_layers = [\n",
    "            Encoder(embed_dim, mlp_dim, num_heads, dropout_rate, attention_dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Output\n",
    "        self.norm = LayerNormalization(epsilon=1e-5)\n",
    "        self.pool = GlobalAveragePooling1D(data_format = 'channels_first')\n",
    "        self.dense_0 = Dense(mlp_dim, activation = 'relu')\n",
    "        self.final_layer = Dense(1, kernel_initializer=\"zeros\", activation = 'sigmoid')\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        expanded_input = tf.cast(tf.tile(tf.expand_dims(inputs, axis=-2), [1, 1, 500,1]), tf.float32)\n",
    "        self.masking_layer.build(expanded_input.shape)\n",
    "        mask = self.masking_layer.compute_mask(expanded_input)\n",
    "        x = self.input_norm(inputs) \n",
    "        x = self.pos_embs(x, training=training)\n",
    "        for layer in self.e_layers:\n",
    "            x = layer(x, training=training , mask = mask)\n",
    "        x = self.norm(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense_0(x)\n",
    "\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380a929",
   "metadata": {},
   "source": [
    "## Train Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8342ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config = {\n",
    "      'epochs': 50,\n",
    "      'num_layers':  3,\n",
    "      'embed_layer_size': 3,\n",
    "      'global_clipnorm' : 3.0,\n",
    "      'fc_layer_size': 256,\n",
    "      'num_heads': 2,\n",
    "      'dropout': 0.1,\n",
    "      'attention_dropout': 0.1,\n",
    "      'optimizer': 'adam',\n",
    "      'amsgrad': False,\n",
    "      'label_smoothing': 0.1,\n",
    "      'learning_rate': 1e-3,\n",
    "      #'weight_decay': {\n",
    "      #    'values': [2.5e-4, 1e-4, 5e-5, 1e-5]\n",
    "      'warmup_steps': 5,\n",
    "      'batch_size': 8}\n",
    "  \n",
    "\n",
    "    # config = wandb.config\n",
    "    \n",
    "    # Generate new model\n",
    "    model = Transformer(\n",
    "      num_layers=config['num_layers'],\n",
    "      embed_dim=config['embed_layer_size']\n",
    "      ,\n",
    "      mlp_dim=config['fc_layer_size'],\n",
    "      num_heads=config['num_heads'],\n",
    "      num_classes=2,\n",
    "      dropout_rate=config['dropout'],\n",
    "      attention_dropout_rate=config['attention_dropout'],\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # adapt on training dataset - must be before model.compile !!!\n",
    "    model.input_norm.adapt(X_train, batch_size=config['batch_size'])\n",
    "    # print(model.input_norm.variables)\n",
    "\n",
    "    # Select optimizer\n",
    "    if config['optimizer'] == \"adam\":\n",
    "      optim = Adam(\n",
    "          global_clipnorm=config['global_clipnorm'],\n",
    "          amsgrad=config['amsgrad'],\n",
    "      )\n",
    "    # elif config.optimizer == \"adamw\":\n",
    "    #   optim = AdamW(\n",
    "    #       weight_decay=config.weight_decay,\n",
    "    #       amsgrad=config.amsgrad,\n",
    "    #       global_clipnorm=config.global_clipnorm,\n",
    "    #       exclude_from_weight_decay=[\"position\"]\n",
    "    #   )\n",
    "    else:\n",
    "      raise ValueError(\"The used optimizer is not in list of available\")\n",
    "\n",
    "    model.compile(\n",
    "      loss= BinaryCrossentropy(label_smoothing=config['label_smoothing']),\n",
    "      optimizer=optim,\n",
    "      metrics=[Recall(), Precision() , AUC(), F1_Score()],\n",
    "    )\n",
    "    \n",
    "    checkpoint_filepath = os.path.join(os.getcwd(), 'tmp/chekcpoint')\n",
    "    model_checkpoint = ModelCheckpoint(filepath = checkpoint_filepath, \n",
    "                                      save_weights_only = True, \n",
    "                                      monitor = 'val_f1_score', \n",
    "                                      mode = 'max', \n",
    "                                      save_best_only = True, \n",
    "                                      verbose = True)\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "      X_train,\n",
    "      y_train,\n",
    "      batch_size=config['batch_size'],\n",
    "      epochs=config['epochs'],\n",
    "      validation_data=(X_val, y_val),\n",
    "      callbacks=[\n",
    "        LearningRateScheduler(cosine_schedule(base_lr=config['learning_rate'], total_steps=config['epochs'], warmup_steps=config['warmup_steps'])),\n",
    "        EarlyStopping(monitor=\"val_loss\", mode='min', min_delta=0.001, patience=5),\n",
    "        model_checkpoint\n",
    "      ],\n",
    "      verbose=1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f06f53c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 23:54:46.039522: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 23:54:46.787618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7372 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:9e:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.6931 - recall: 0.3283 - precision: 0.5625 - auc: 0.5000 - f1_score: 0.4146\n",
      "Epoch 1: val_f1_score improved from -inf to 0.71429, saving model to /home/Students/bgu9/KD_Transformer/tmp/chekcpoint\n",
      "82/82 [==============================] - 9s 52ms/step - loss: 0.6930 - recall: 0.3323 - precision: 0.5670 - auc: 0.5000 - f1_score: 0.4190 - val_loss: 0.6920 - val_recall: 0.7627 - val_precision: 0.6716 - val_auc: 0.5825 - val_f1_score: 0.7143 - lr: 2.0000e-04\n",
      "Epoch 2/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.6804 - recall: 0.7364 - precision: 0.6550 - auc: 0.6995 - f1_score: 0.6933\n",
      "Epoch 2: val_f1_score improved from 0.71429 to 0.71698, saving model to /home/Students/bgu9/KD_Transformer/tmp/chekcpoint\n",
      "82/82 [==============================] - 3s 42ms/step - loss: 0.6808 - recall: 0.7341 - precision: 0.6515 - auc: 0.6940 - f1_score: 0.6903 - val_loss: 0.6456 - val_recall: 0.6441 - val_precision: 0.8085 - val_auc: 0.8446 - val_f1_score: 0.7170 - lr: 2.0000e-04\n",
      "Epoch 3/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.6187 - recall: 0.6951 - precision: 0.7261 - auc: 0.7699 - f1_score: 0.7103\n",
      "Epoch 3: val_f1_score improved from 0.71698 to 0.76364, saving model to /home/Students/bgu9/KD_Transformer/tmp/chekcpoint\n",
      "82/82 [==============================] - 3s 42ms/step - loss: 0.6188 - recall: 0.6949 - precision: 0.7278 - auc: 0.7691 - f1_score: 0.7110 - val_loss: 0.5470 - val_recall: 0.7119 - val_precision: 0.8235 - val_auc: 0.8756 - val_f1_score: 0.7636 - lr: 2.0000e-04\n",
      "Epoch 4/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.5758 - recall: 0.6394 - precision: 0.7617 - auc: 0.8135 - f1_score: 0.6952\n",
      "Epoch 4: val_f1_score did not improve from 0.76364\n",
      "82/82 [==============================] - 3s 41ms/step - loss: 0.5747 - recall: 0.6405 - precision: 0.7626 - auc: 0.8152 - f1_score: 0.6962 - val_loss: 0.5267 - val_recall: 0.6441 - val_precision: 0.8444 - val_auc: 0.8913 - val_f1_score: 0.7308 - lr: 2.0000e-04\n",
      "Epoch 5/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.5636 - recall: 0.6879 - precision: 0.7542 - auc: 0.8276 - f1_score: 0.7195\n",
      "Epoch 5: val_f1_score did not improve from 0.76364\n",
      "82/82 [==============================] - 3s 41ms/step - loss: 0.5650 - recall: 0.6888 - precision: 0.7525 - auc: 0.8264 - f1_score: 0.7192 - val_loss: 0.5054 - val_recall: 0.6949 - val_precision: 0.8200 - val_auc: 0.8880 - val_f1_score: 0.7523 - lr: 2.0000e-04\n",
      "Epoch 6/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.5549 - recall: 0.6727 - precision: 0.7577 - auc: 0.8321 - f1_score: 0.7127\n",
      "Epoch 6: val_f1_score did not improve from 0.76364\n",
      "82/82 [==============================] - 3s 41ms/step - loss: 0.5546 - recall: 0.6737 - precision: 0.7585 - auc: 0.8323 - f1_score: 0.7136 - val_loss: 0.5021 - val_recall: 0.6949 - val_precision: 0.8200 - val_auc: 0.8912 - val_f1_score: 0.7523 - lr: 2.0000e-04\n",
      "Epoch 7/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.5374 - recall: 0.6788 - precision: 0.7832 - auc: 0.8572 - f1_score: 0.7273\n",
      "Epoch 7: val_f1_score did not improve from 0.76364\n",
      "82/82 [==============================] - 3s 41ms/step - loss: 0.5369 - recall: 0.6798 - precision: 0.7812 - auc: 0.8573 - f1_score: 0.7270 - val_loss: 0.4859 - val_recall: 0.7288 - val_precision: 0.7818 - val_auc: 0.9024 - val_f1_score: 0.7544 - lr: 2.0000e-04\n",
      "Epoch 8/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.5302 - recall: 0.7121 - precision: 0.7556 - auc: 0.8550 - f1_score: 0.7332\n",
      "Epoch 8: val_f1_score did not improve from 0.76364\n",
      "82/82 [==============================] - 3s 40ms/step - loss: 0.5296 - recall: 0.7130 - precision: 0.7540 - auc: 0.8554 - f1_score: 0.7329 - val_loss: 0.4819 - val_recall: 0.6780 - val_precision: 0.8333 - val_auc: 0.9036 - val_f1_score: 0.7477 - lr: 2.0000e-04\n",
      "Epoch 9/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.5098 - recall: 0.7195 - precision: 0.7919 - auc: 0.8750 - f1_score: 0.7540\n",
      "Epoch 9: val_f1_score improved from 0.76364 to 0.78571, saving model to /home/Students/bgu9/KD_Transformer/tmp/chekcpoint\n",
      "82/82 [==============================] - 3s 43ms/step - loss: 0.5107 - recall: 0.7160 - precision: 0.7926 - auc: 0.8743 - f1_score: 0.7524 - val_loss: 0.4821 - val_recall: 0.7458 - val_precision: 0.8302 - val_auc: 0.8995 - val_f1_score: 0.7857 - lr: 2.0000e-04\n",
      "Epoch 10/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.5117 - recall: 0.6970 - precision: 0.8244 - auc: 0.8739 - f1_score: 0.7553\n",
      "Epoch 10: val_f1_score improved from 0.78571 to 0.78947, saving model to /home/Students/bgu9/KD_Transformer/tmp/chekcpoint\n",
      "82/82 [==============================] - 3s 43ms/step - loss: 0.5108 - recall: 0.6979 - precision: 0.8250 - auc: 0.8745 - f1_score: 0.7561 - val_loss: 0.4672 - val_recall: 0.7627 - val_precision: 0.8182 - val_auc: 0.9069 - val_f1_score: 0.7895 - lr: 2.0000e-04\n",
      "Epoch 11/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4934 - recall: 0.7781 - precision: 0.7975 - auc: 0.8855 - f1_score: 0.7877\n",
      "Epoch 11: val_f1_score improved from 0.78947 to 0.83871, saving model to /home/Students/bgu9/KD_Transformer/tmp/chekcpoint\n",
      "82/82 [==============================] - 4s 43ms/step - loss: 0.4933 - recall: 0.7764 - precision: 0.7981 - auc: 0.8856 - f1_score: 0.7871 - val_loss: 0.4686 - val_recall: 0.8814 - val_precision: 0.8000 - val_auc: 0.9142 - val_f1_score: 0.8387 - lr: 2.0000e-04\n",
      "Epoch 12/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4917 - recall: 0.8030 - precision: 0.7840 - auc: 0.8881 - f1_score: 0.7934\n",
      "Epoch 12: val_f1_score did not improve from 0.83871\n",
      "82/82 [==============================] - 3s 40ms/step - loss: 0.4919 - recall: 0.8036 - precision: 0.7824 - auc: 0.8878 - f1_score: 0.7928 - val_loss: 0.4625 - val_recall: 0.8475 - val_precision: 0.8197 - val_auc: 0.9168 - val_f1_score: 0.8333 - lr: 2.0000e-04\n",
      "Epoch 13/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4743 - recall: 0.8459 - precision: 0.8000 - auc: 0.9007 - f1_score: 0.8223\n",
      "Epoch 13: val_f1_score did not improve from 0.83871\n",
      "82/82 [==============================] - 3s 40ms/step - loss: 0.4735 - recall: 0.8459 - precision: 0.8000 - auc: 0.9016 - f1_score: 0.8223 - val_loss: 0.4550 - val_recall: 0.7627 - val_precision: 0.8182 - val_auc: 0.9131 - val_f1_score: 0.7895 - lr: 2.0000e-04\n",
      "Epoch 14/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4695 - recall: 0.8157 - precision: 0.8232 - auc: 0.9005 - f1_score: 0.8194\n",
      "Epoch 14: val_f1_score did not improve from 0.83871\n",
      "82/82 [==============================] - 3s 40ms/step - loss: 0.4707 - recall: 0.8157 - precision: 0.8182 - auc: 0.8995 - f1_score: 0.8169 - val_loss: 0.4552 - val_recall: 0.7966 - val_precision: 0.8393 - val_auc: 0.9125 - val_f1_score: 0.8174 - lr: 2.0000e-04\n",
      "Epoch 15/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4618 - recall: 0.8273 - precision: 0.8125 - auc: 0.9055 - f1_score: 0.8198\n",
      "Epoch 15: val_f1_score did not improve from 0.83871\n",
      "82/82 [==============================] - 3s 39ms/step - loss: 0.4607 - recall: 0.8278 - precision: 0.8131 - auc: 0.9063 - f1_score: 0.8204 - val_loss: 0.4407 - val_recall: 0.8136 - val_precision: 0.8421 - val_auc: 0.9218 - val_f1_score: 0.8276 - lr: 2.0000e-04\n",
      "Epoch 16/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4517 - recall: 0.8394 - precision: 0.8293 - auc: 0.9117 - f1_score: 0.8343\n",
      "Epoch 16: val_f1_score improved from 0.83871 to 0.84210, saving model to /home/Students/bgu9/KD_Transformer/tmp/chekcpoint\n",
      "82/82 [==============================] - 3s 42ms/step - loss: 0.4508 - recall: 0.8399 - precision: 0.8299 - auc: 0.9124 - f1_score: 0.8348 - val_loss: 0.4432 - val_recall: 0.8136 - val_precision: 0.8727 - val_auc: 0.9190 - val_f1_score: 0.8421 - lr: 2.0000e-04\n",
      "Epoch 17/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4476 - recall: 0.8450 - precision: 0.8299 - auc: 0.9152 - f1_score: 0.8373\n",
      "Epoch 17: val_f1_score did not improve from 0.84210\n",
      "82/82 [==============================] - 3s 41ms/step - loss: 0.4471 - recall: 0.8459 - precision: 0.8309 - auc: 0.9156 - f1_score: 0.8383 - val_loss: 0.4435 - val_recall: 0.7797 - val_precision: 0.8846 - val_auc: 0.9248 - val_f1_score: 0.8288 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4307 - recall: 0.8567 - precision: 0.8265 - auc: 0.9271 - f1_score: 0.8413\n",
      "Epoch 18: val_f1_score improved from 0.84210 to 0.84956, saving model to /home/Students/bgu9/KD_Transformer/tmp/chekcpoint\n",
      "82/82 [==============================] - 4s 43ms/step - loss: 0.4332 - recall: 0.8520 - precision: 0.8270 - auc: 0.9254 - f1_score: 0.8393 - val_loss: 0.4348 - val_recall: 0.8136 - val_precision: 0.8889 - val_auc: 0.9234 - val_f1_score: 0.8496 - lr: 2.0000e-04\n",
      "Epoch 19/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4252 - recall: 0.8606 - precision: 0.8452 - auc: 0.9306 - f1_score: 0.8529\n",
      "Epoch 19: val_f1_score improved from 0.84956 to 0.85217, saving model to /home/Students/bgu9/KD_Transformer/tmp/chekcpoint\n",
      "82/82 [==============================] - 3s 42ms/step - loss: 0.4248 - recall: 0.8610 - precision: 0.8457 - auc: 0.9308 - f1_score: 0.8533 - val_loss: 0.4388 - val_recall: 0.8305 - val_precision: 0.8750 - val_auc: 0.9230 - val_f1_score: 0.8522 - lr: 2.0000e-04\n",
      "Epoch 20/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4252 - recall: 0.8602 - precision: 0.8576 - auc: 0.9304 - f1_score: 0.8589\n",
      "Epoch 20: val_f1_score did not improve from 0.85217\n",
      "82/82 [==============================] - 3s 40ms/step - loss: 0.4245 - recall: 0.8610 - precision: 0.8584 - auc: 0.9308 - f1_score: 0.8597 - val_loss: 0.4460 - val_recall: 0.7797 - val_precision: 0.9200 - val_auc: 0.9298 - val_f1_score: 0.8440 - lr: 2.0000e-04\n",
      "Epoch 21/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4233 - recall: 0.8445 - precision: 0.8549 - auc: 0.9296 - f1_score: 0.8497\n",
      "Epoch 21: val_f1_score improved from 0.85217 to 0.85714, saving model to /home/Students/bgu9/KD_Transformer/tmp/chekcpoint\n",
      "82/82 [==============================] - 3s 42ms/step - loss: 0.4228 - recall: 0.8459 - precision: 0.8563 - auc: 0.9300 - f1_score: 0.8511 - val_loss: 0.4356 - val_recall: 0.8644 - val_precision: 0.8500 - val_auc: 0.9265 - val_f1_score: 0.8571 - lr: 2.0000e-04\n",
      "Epoch 22/50\n",
      "81/82 [============================>.] - ETA: 0s - loss: 0.4279 - recall: 0.8693 - precision: 0.8242 - auc: 0.9281 - f1_score: 0.8462\n",
      "Epoch 22: val_f1_score did not improve from 0.85714\n",
      "82/82 [==============================] - 3s 40ms/step - loss: 0.4270 - recall: 0.8701 - precision: 0.8252 - auc: 0.9287 - f1_score: 0.8471 - val_loss: 0.4339 - val_recall: 0.8305 - val_precision: 0.8596 - val_auc: 0.9277 - val_f1_score: 0.8448 - lr: 2.0000e-04\n",
      "Epoch 23/50\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4215 - recall: 0.8671 - precision: 0.8247 - auc: 0.9323 - f1_score: 0.8454\n",
      "Epoch 23: val_f1_score did not improve from 0.85714\n",
      "82/82 [==============================] - 3s 41ms/step - loss: 0.4215 - recall: 0.8671 - precision: 0.8247 - auc: 0.9323 - f1_score: 0.8454 - val_loss: 0.4432 - val_recall: 0.8475 - val_precision: 0.8197 - val_auc: 0.9268 - val_f1_score: 0.8333 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "    model = None\n",
    "    dataset_path = os.path.join(os.getcwd(), 'fall_detection_dataset.npz')\n",
    "    f = np.load(dataset_path)\n",
    "    signals = f['trials']\n",
    "\n",
    "    labels = f['labels']\n",
    "\n",
    "    # split to train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        signals, labels, test_size=0.15, random_state=9, stratify=labels\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.15, random_state=9, stratify=y_train\n",
    "    )\n",
    "\n",
    "    # sweep_id = wandb.sweep(sweep_config, project=\"KD_Transformer\")\n",
    "    with tf.device('/gpu:0'):\n",
    "      model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1ead83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f9de0363a90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(check_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01c9868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 19ms/step - loss: 0.4705 - recall: 0.8261 - precision: 0.8143 - auc: 0.8984 - f1_score: 0.8201\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=8 , steps = X_test.shape[0] / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5b4d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = 2/((1/score[1]) + (1/score[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "974cdac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8472222344846358"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2898867",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_path = os.path.join(os.getcwd(), 'tmp/chekcpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4d41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
